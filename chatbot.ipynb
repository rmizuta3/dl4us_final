{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "2muVXIfhcpfJ",
    "outputId": "a716c4d8-174e-4f85-c698-163c1dbbcdc6"
   },
   "outputs": [],
   "source": [
    "#環境がcolabの場合\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9i8EUzbvWn2H"
   },
   "outputs": [],
   "source": [
    "#f1=open('drive/My Drive/call.txt')\n",
    "#f2=open('drive/My Drive/reply.txt')\n",
    "f1=open('call.txt')\n",
    "f2=open('reply.txt')\n",
    "call_limited=[]\n",
    "reply_limited=[]\n",
    "for line1,line2 in zip(f1,f2):\n",
    "    if (len(line2) <= 50) and (len(line1) <= 50):\n",
    "        if (\"…\" not in line1) and (\"…\" not in line2):\n",
    "            call_limited.append(line1)\n",
    "            reply_limited.append(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8b4YkoQYb0f3",
    "outputId": "f7d69a01-6a6c-44e7-f334-0601f30f57c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data2(arr):\n",
    "    tokenizer = Tokenizer(filters=\"\")\n",
    "    whole_texts = []\n",
    "    count=0\n",
    "    for line in arr:\n",
    "        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n",
    "    tokenizer.fit_on_texts(whole_texts)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n",
    "\n",
    "x_train, tokenizer_en = load_data2(call_limited)\n",
    "y_train, tokenizer_ja = load_data2(reply_limited)\n",
    "\n",
    "en_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "ja_vocab_size = len(tokenizer_ja.word_index) + 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.02, random_state=42)\n",
    "\n",
    "# パディング\n",
    "x_train = pad_sequences(x_train, padding='post')\n",
    "y_train = pad_sequences(y_train, padding='post')\n",
    "\n",
    "seqX_len = len(x_train[0])\n",
    "seqY_len = len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bU6DDg2eb5zE"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Permute, Activation, Embedding, Dense, LSTM, concatenate, dot\n",
    "from keras import backend as K\n",
    "\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "att_dim = 256\n",
    "\n",
    "# 符号化器\n",
    "encoder_inputs = Input(shape=(seqX_len,))\n",
    "encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs)\n",
    "encoded_seq, *encoder_states = LSTM(hid_dim, return_sequences=True, return_state=True)(encoder_embedded)\n",
    "\n",
    "# 復号化器（encoder_statesを初期状態として指定）\n",
    "decoder_inputs = Input(shape=(seqY_len,))\n",
    "decoder_embedding = Embedding(ja_vocab_size, emb_dim)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True)\n",
    "decoded_seq, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "\n",
    "# Attention\n",
    "score_dense = Dense(hid_dim)\n",
    "score = score_dense(decoded_seq)                        # shape: (seqY_len, hid_dim) -> (seqY_len, hid_dim)\n",
    "score = dot([score, encoded_seq], axes=(2,2))           # shape: [(seqY_len, hid_dim), (seqX_len, hid_dim)] -> (seqY_len, seqX_len)\n",
    "attention = Activation('softmax')(score)                # shape: (seqY_len, seqX_len) -> (seqY_len, seqX_len)\n",
    "context = dot([attention, encoded_seq], axes=(2,1))     # shape: [(seqY_len, seqX_len), (seqX_len, hid_dim)] -> (seqY_len, hid_dim)\n",
    "concat = concatenate([context, decoded_seq], axis=2)    # shape: [(seqY_len, hid_dim), (seqY_len, hid_dim)] -> (seqY_len, 2*hid_dim)\n",
    "attention_dense = Dense(att_dim, activation='tanh')\n",
    "attentional = attention_dense(concat)                   # shape: (seqY_len, 2*hid_dim) -> (seqY_len, att_dim)\n",
    "output_dense = Dense(ja_vocab_size, activation='softmax')\n",
    "outputs = output_dense(attentional)                     # shape: (seqY_len, att_dim) -> (seqY_len, ja_vocab_size)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "#model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "XbtVES7Fb8O3",
    "outputId": "89b0649d-35ca-4109-898c-405a68eb8c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 314528 samples, validate on 78632 samples\n",
      "Epoch 1/5\n",
      "314528/314528 [==============================] - 2012s 6ms/step - loss: 1.8961 - val_loss: 1.6303\n",
      "Epoch 2/5\n",
      "314528/314528 [==============================] - 2010s 6ms/step - loss: 1.5657 - val_loss: 1.5409\n",
      "Epoch 3/5\n",
      "314528/314528 [==============================] - 2011s 6ms/step - loss: 1.4905 - val_loss: 1.4989\n",
      "Epoch 4/5\n",
      "314528/314528 [==============================] - 2012s 6ms/step - loss: 1.4371 - val_loss: 1.4781\n",
      "Epoch 5/5\n",
      "314528/314528 [==============================] - 2013s 6ms/step - loss: 1.3925 - val_loss: 1.4590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5c2e47278>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
    "\n",
    "#cp_cb = ModelCheckpoint(filepath = fpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=128, epochs=5, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-vnH3iNzgyZL",
    "outputId": "8cd6deae-28bf-453f-c5d2-31ec96d1b8eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save(\"chatbot_u50_e3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KnceAwdRhQwT",
    "outputId": "22e2e4b2-3c00-4730-b4b3-efc5427cf083"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive/My Drive/chatbot_u50_e3.model'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import shutil\n",
    "#shutil.copyfile(\"/content/chatbot_u50_e3.model\", \"drive/My Drive/chatbot_u50_e3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXu5eIT3YJk6"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#model=load_model(\"drive/My Drive/chatbot_u50_e3.model\")\n",
    "model.load_weights(\"drive/My Drive/chatbot_u50_e3.model\")\n",
    "#model.load_weights(\"drive/My Drive/chatbot_50v2_e5.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoASF0liIFt3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Permute, Activation, Embedding, Dense, LSTM, concatenate, dot\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUV10p5ib982"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoded_seq]+encoder_states)\n",
    "\n",
    "decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
    "\n",
    "decoder_inputs = Input(shape=(1,))\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoded_seq, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoded_seq] + decoder_states)\n",
    "\n",
    "# Attention\n",
    "encoded_seq_in, decoded_seq_in = Input(shape=(seqX_len, hid_dim)), Input(shape=(1, hid_dim))\n",
    "score = score_dense(decoded_seq_in)\n",
    "score = dot([score, encoded_seq_in], axes=(2,2))\n",
    "attention = Activation('softmax')(score)\n",
    "context = dot([attention, encoded_seq_in], axes=(2,1))\n",
    "concat = concatenate([context, decoded_seq_in], axis=2)\n",
    "attentional = attention_dense(concat)\n",
    "attention_outputs = output_dense(attentional)\n",
    "\n",
    "attention_model = Model([encoded_seq_in, decoded_seq_in], [attention_outputs, attention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u221gG0wb_7_"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, bos_eos, max_output_length = 1000):\n",
    "    encoded_seq, *states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"に対応するインデックス\n",
    "    output_seq = bos_eos[0][:]\n",
    "    attention_seq = np.empty((0,len(input_seq[0])))\n",
    "    \n",
    "    while True:\n",
    "        decoded_seq, *states_value = decoder_model.predict([target_seq] + states_value)\n",
    "        output_tokens, attention = attention_model.predict([encoded_seq, decoded_seq])\n",
    "        sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
    "        output_seq += sampled_token_index\n",
    "        attention_seq = np.append(attention_seq, attention[0], axis=0)\n",
    "        \n",
    "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array(sampled_token_index)\n",
    "\n",
    "    return output_seq, attention_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhI1EgivIaaP"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "detokenizer_en = dict(map(reversed, tokenizer_en.word_index.items()))\n",
    "detokenizer_ja = dict(map(reversed, tokenizer_ja.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aOYkVluvIcGm",
    "outputId": "cb4c63cf-6786-4866-fcee-6011a27cb675"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce952d572cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"好き な 食べ物 は ？\"\u001b[0m \u001b[0;31m#分かち書き後の入力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<s> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" </s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbos_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_ja\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ce952d572cd6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"好き な 食べ物 は ？\"\u001b[0m \u001b[0;31m#分かち書き後の入力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<s> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" </s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbos_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_ja\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_en' is not defined"
     ]
    }
   ],
   "source": [
    "#回答確認用\n",
    "word=\"好き な 食べ物 は ？\" #分かち書き後の入力\n",
    "word1=\"<s> \" + word.strip() + \" </s>\"\n",
    "tmp=[[tokenizer_en.texts_to_sequences([i])[0][0] for i in word1.split()]]\n",
    "input_seq =pad_sequences(tmp, 26, padding='post')\n",
    "bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "output_seq, attention_seq = decode_sequence(input_seq, bos_eos)\n",
    "print(\"発話文:「\",word,\"」\")\n",
    "print('回答文:「', ' '.join([detokenizer_ja[i] for i in output_seq[1:-1]]),\"」\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
